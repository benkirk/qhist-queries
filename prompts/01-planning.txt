I have access to some machines 'casper' and 'derecho' where I can run
the command 'qhist' - examine and run ./bin/sample.sh for some
examples.  qhist returns historical information for jobs run on
supercomputing resources. I can access these machines directly but
right now an using ssh.

qhist can return data in several formats, csv, json, or user-specified
formats.

qhist can look at a particular specified time range, for example one
single day.

I would like to create a local database (ideally SQLite bit also I am
open to a MySQL container if that is a much better solution for some
reason.  I would like you to recommend a database schema structure for
one or several tables for working with these data.  I plan to do
historical usage analysis including usage by user, account,
computational resource usage, queues, etc...  as well as analize wait
times, run times, memory usage vs. request, etc..  I do not need you
to worry about those queries now but consider these use cases when
proposing the schema. Casper and Derecho have different 'epochs', and
likely should be separate tables, but I am open to discussion.


I would also like to create a SQLAlchemy ORM framework matching the
new database.  I am currently in an active conda environment that has
the necessary prerequisites installed, mysql, sqlite, etc...

I would like a simple process I can run at intervals to populate the
database with new entries, for example run an update process daily or
weekly or whenever to get new entries. Ideally this process would use
the developed ORMS.

Python, bash, make are preferred tools for this process.
